# Training Model 3 and generating prediction

library(readr)
library(xgboost)
library(pROC)

users<-read_csv("users14.csv")
test<-read_csv("users15.csv")
feature.names<-c('GEN','Age1','Age2','Age3','CAP','VIL','INC','WY','JustHave','LongHave','Lost','AMT','AMTb','NetFreq','PosFreq','PosSD') 

train_eval<-users[,feature.names]
train_target<-setnames(data.frame(users[,c('ApplyH2')]),'target')

h <-sample(nrow(train_target),40000)

dval   <-xgb.DMatrix(data=data.matrix(train_eval[h,]),label=train_target$target[h])
dtrain <-xgb.DMatrix(data=data.matrix(train_eval[-h,]),label=train_target$target[-h])

xgb_watchlist <-list(val=dval,train=dtrain) 
xgb_params <- list(  objective           = "binary:logistic", 
                     booster = "gbtree",
                     eval_metric = "auc",
                     eta                 = 0.05,  
                     max_depth           = 4,  
                     subsample           = 0.65,       
                     colsample_bytree    = 0.65                           
)
xgb_model <- xgb.train(
  params              = xgb_params, 
  data                = dtrain, 
  nrounds             = 197,
  verbose             = 1,  
  watchlist           = xgb_watchlist,
  print.every.n       = 10,
  maximize            = FALSE
)

# Prediction on test
test_features<-test[,feature.names]
predictions <- predict(xgb_model, data.matrix(test_features))

# watch cluster stats
test$PRED<-predictions
sum(test$PRED)
sum(test$PRED[test$JustHave==1])
sum(test$PRED[test$LongHave==1]) 
sum(test$PRED[test$Lost==1])  

